---
- name: Deploy Apache Spark Common Components
  hosts: all
  become: yes
  vars:
    install_dir: /opt
    spark_user: spark
    spark_group: spark
    jdk_version: "jdk1.8.0_202"
    hadoop_version: "hadoop-2.7.1"
    spark_version: "spark-2.4.3-bin-hadoop2.7"
    
    jdk_url: "https://s3.hn-1.cloud.cmctelecom.vn/fileio/jdk-8u202-linux-x64.tar.gz"
    hadoop_url: "https://s3.hn-1.cloud.cmctelecom.vn/fileio/hadoop-2.7.1.tar.gz"
    spark_url: "https://s3.hn-1.cloud.cmctelecom.vn/fileio/spark-2.4.3-bin-hadoop2.7.tgz"
    
    spark_master_host: "{{ hostvars[groups['spark_master'] | default(['']) | first]['ansible_host'] | default('10.0.1.20') if groups['spark_master'] | default([]) else '10.0.1.20' }}"
    spark_master_port: 7077
    spark_master_webui_port: 8080
    spark_worker_webui_port: 8081

  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install packages
      apt:
        name:
          - python3
          - python3-pip
          - wget
          - curl
          - tar
          - ssh
          - rsync
          - net-tools
        state: present

    - name: Config SSH
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: "{{ item.regexp }}"
        line: "{{ item.line }}"
        state: present
      loop:
        - { regexp: '^#?PasswordAuthentication', line: 'PasswordAuthentication no' }
        - { regexp: '^#?PubkeyAuthentication', line: 'PubkeyAuthentication yes' }
        - { regexp: '^#?ChallengeResponseAuthentication', line: 'ChallengeResponseAuthentication no' }
      notify: restart sshd

    - name: Create Spark group
      group:
        name: "{{ spark_group }}"
        state: present

    - name: Create Spark user
      user:
        name: "{{ spark_user }}"
        group: "{{ spark_group }}"
        shell: /bin/bash
        home: "/home/{{ spark_user }}"
        create_home: yes
        state: present

    - name: Create installation directory
      file:
        path: "{{ install_dir }}"
        state: directory
        mode: '0755'

    - name: Check if JDK is already installed
      stat:
        path: "{{ install_dir }}/{{ jdk_version }}"
      register: jdk_installed

    - name: Download JDK
      get_url:
        url: "{{ jdk_url }}"
        dest: "/tmp/jdk.tar.gz"
        mode: '0644'
      when: not jdk_installed.stat.exists

    - name: Extract JDK
      unarchive:
        src: "/tmp/jdk.tar.gz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ install_dir }}/{{ jdk_version }}"
      when: not jdk_installed.stat.exists

    - name: Create Java symbolic link
      file:
        src: "{{ install_dir }}/{{ jdk_version }}"
        dest: "{{ install_dir }}/java"
        state: link

    - name: Check if Hadoop is already installed
      stat:
        path: "{{ install_dir }}/{{ hadoop_version }}"
      register: hadoop_installed

    - name: Download Hadoop
      get_url:
        url: "{{ hadoop_url }}"
        dest: "/tmp/hadoop.tar.gz"
        mode: '0644'
      when: not hadoop_installed.stat.exists

    - name: Extract Hadoop
      unarchive:
        src: "/tmp/hadoop.tar.gz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ install_dir }}/{{ hadoop_version }}"
      when: not hadoop_installed.stat.exists

    - name: Create Hadoop symbolic link
      file:
        src: "{{ install_dir }}/{{ hadoop_version }}"
        dest: "{{ install_dir }}/hadoop"
        state: link

    - name: Check if Spark is already installed
      stat:
        path: "{{ install_dir }}/{{ spark_version }}"
      register: spark_installed

    - name: Download Spark
      get_url:
        url: "{{ spark_url }}"
        dest: "/tmp/spark.tgz"
        mode: '0644'
      when: not spark_installed.stat.exists

    - name: Extract Spark
      unarchive:
        src: "/tmp/spark.tgz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ install_dir }}/{{ spark_version }}"
      when: not spark_installed.stat.exists

    - name: Create Spark symbolic link
      file:
        src: "{{ install_dir }}/{{ spark_version }}"
        dest: "{{ install_dir }}/spark"
        state: link

    - name: Change Ownership
      file:
        path: "{{ item }}"
        owner: "{{ spark_user }}"
        group: "{{ spark_group }}"
        recurse: yes
      loop:
        - "{{ install_dir }}/{{ jdk_version }}"
        - "{{ install_dir }}/{{ hadoop_version }}"
        - "{{ install_dir }}/{{ spark_version }}"

    - name: Configure environment variables for Spark user
      blockinfile:
        path: "/home/{{ spark_user }}/.bashrc"
        block: |
          export JAVA_HOME={{ install_dir }}/java
          export HADOOP_HOME={{ install_dir }}/hadoop
          export SPARK_HOME={{ install_dir }}/spark
          export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin
          export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
          export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
        marker: "# {mark} ANSIBLE MANAGED BLOCK - Spark Environment"
        create: yes
        owner: "{{ spark_user }}"
        group: "{{ spark_group }}"

    - name: Configure system-wide environment variables
      copy:
        dest: /etc/profile.d/spark.sh
        content: |
          export JAVA_HOME={{ install_dir }}/java
          export HADOOP_HOME={{ install_dir }}/hadoop
          export SPARK_HOME={{ install_dir }}/spark
          export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin
          export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
          export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
        mode: '0644'

    - name: Create Spark log directory
      file:
        path: /var/log/spark
        state: directory
        owner: "{{ spark_user }}"
        group: "{{ spark_group }}"
        mode: '0755'

    - name: Create Spark work directory
      file:
        path: /var/lib/spark/work
        state: directory
        owner: "{{ spark_user }}"
        group: "{{ spark_group }}"
        mode: '0755'

    - name: Create Spark pid directory
      file:
        path: /var/run/spark
        state: directory
        owner: "{{ spark_user }}"
        group: "{{ spark_group }}"
        mode: '0755'

    - name: Configure spark-env.sh
      copy:
        dest: "{{ install_dir }}/spark/conf/spark-env.sh"
        content: |
          #!/usr/bin/env bash
          export JAVA_HOME={{ install_dir }}/java
          export HADOOP_CONF_DIR={{ install_dir }}/hadoop/etc/hadoop
          export SPARK_MASTER_HOST={{ spark_master_host }}
          export SPARK_MASTER_PORT={{ spark_master_port }}
          export SPARK_MASTER_WEBUI_PORT={{ spark_master_webui_port }}
          export SPARK_WORKER_WEBUI_PORT={{ spark_worker_webui_port }}
          export SPARK_LOG_DIR=/var/log/spark
          export SPARK_WORKER_DIR=/var/lib/spark/work
          export SPARK_PID_DIR=/var/run/spark
          export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
        owner: "{{ spark_user }}"
        group: "{{ spark_group }}"
        mode: '0755'

    - name: Add Spark environment variables to .bashrc
      blockinfile:
        path: "~/.bashrc"
        block: |
          export JAVA_HOME=/opt/java-8-openjdk-amd64
          export HADOOP_HOME=/opt/hadoop-2.7.1
          export SPARK_HOME=/opt/spark-2.4.3-bin-hadoop2.7
          export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
        marker: "# {mark} ANSIBLE MANAGED BLOCK - Spark Environment"
        create: yes

    - name: Configure spark-defaults.conf
      copy:
        dest: "{{ install_dir }}/spark/conf/spark-defaults.conf"
        content: |
          spark.master                     spark://{{ spark_master_host }}:{{ spark_master_port }}
          spark.eventLog.enabled           true
          spark.eventLog.dir               file:///var/log/spark/events
          spark.history.fs.logDirectory    file:///var/log/spark/events
          spark.serializer                 org.apache.spark.serializer.KryoSerializer
          spark.driver.memory              1g
          spark.executor.memory            1g
        owner: "{{ spark_user }}"
        group: "{{ spark_group }}"
        mode: '0644'

    - name: Create Spark event log directory
      file:
        path: /var/log/spark/events
        state: directory
        owner: "{{ spark_user }}"
        group: "{{ spark_group }}"
        mode: '0755'

    - name: Clean up temporary installation files
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /tmp/jdk.tar.gz
        - /tmp/hadoop.tar.gz
        - /tmp/spark.tgz

  handlers:
    - name: restart sshd
      service:
        name: sshd
        state: restarted
